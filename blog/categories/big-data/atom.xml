<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Big Data | Spaghetti Optimization]]></title>
  <link href="http://stegua.github.com/blog/categories/big-data/atom.xml" rel="self"/>
  <link href="http://stegua.github.com/"/>
  <updated>2019-02-01T17:30:01+01:00</updated>
  <id>http://stegua.github.com/</id>
  <author>
    <name><![CDATA[Stefano Gualandi]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Big Data and Convex Optimization]]></title>
    <link href="http://stegua.github.com/blog/2014/09/27/big-data-and-convex-optimization/"/>
    <updated>2014-09-27T16:38:00+02:00</updated>
    <id>http://stegua.github.com/blog/2014/09/27/big-data-and-convex-optimization</id>
    <content type="html"><![CDATA[<p>In the last months, I came several times across different definitions of <strong>Big Data</strong>.
However, when someone asks me what Big Data means in practice, I am never
able to give a satisfactory explanation. Indeed, you can easily find a flood
of posts on twitter, blogs, newspaper, and even scientific journals and conferences,
but I always kept feeling that <strong>Big Data</strong> is a buzzword.</p>

<p>By sheer serendipity, this morning I came across <a href="http://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf">three paragraphs</a> clearly
stating the importance of Big Data from a scientific standpoint, that I like to <strong>cross-post</strong> here (the following paragraphs appear in the introduction of [1]):</p>

<p><em>In all applied fields, it is now commonplace to attack problems through data analysis, particularly through the use of statistical and machine learning algorithms on what are often large datasets. In industry, this trend has been referred to as ‘Big Data’, and it has had a significant impact in areas as varied as artificial intelligence, internet applications, computational biology, medicine, finance, marketing, journalism, network analysis, and logistics.</em></p>

<p><em>Though these problems arise in diverse application domains, they share some key characteristics. First, the datasets are often extremely large, consisting of hundreds of millions or billions of training examples; second, the data is often very high-dimensional, because it is now possible to measure and store very detailed information about each example; and third, because of the large scale of many applications, the data is often stored or even collected in a distributed manner. As a result, it has become of central importance to develop algorithms that are both rich enough to capture the complexity of modern data, and scalable enough to process huge datasets in a parallelized or fully decentralized fashion. Indeed, some researchers have suggested that even highly complex and structured problems may succumb most easily to relatively simple models trained on vast datasets.</em></p>

<blockquote>
  <p>Many such problems can be posed in the framework of <strong>Convex Optimization</strong>. </p>
</blockquote>

<p><em>Given the significant work on decomposition methods and decentralized algorithms in the optimization community, it is natural to look to parallel optimization algorithms as a mechanism for solving large-scale statistical tasks. This approach also has the benefit that one algorithm could be flexible enough to solve many problems.</em></p>

<p>Even if I am not an expert of Convex Optimization [2], I do have my own <strong>mathematical optimization</strong> bias. 
Likely, you may have a different opinion (that I am always happy to hear), but, honestly, the above paragraphs
are the best content that I have read so far about <strong>Big Data</strong>.</p>

<h3 id="references">References</h3>

<p>[1] S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein.
<em>Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers</em>.
Foundations and Trends in Machine Learning. Vol. 3, No. 1 (2010) 1–122. <a href="http://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf">[pdf]</a></p>

<p>[2] If you like to have a speedy overview of <em>Convex Optimization</em>, you may read a <a href="https://www.ibm.com/developerworks/community/blogs/jfp/entry/convex_optimization?lang=en">J.F. Puget’s blog post</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Public Transport and Big Data]]></title>
    <link href="http://stegua.github.com/blog/2013/11/17/public-transport-and-big-data/"/>
    <updated>2013-11-17T14:10:00+01:00</updated>
    <id>http://stegua.github.com/blog/2013/11/17/public-transport-and-big-data</id>
    <content type="html"><![CDATA[<p><strong>Big Data</strong> is nowadays a <em>buzzword</em>. A simple query for “Big Data” on Google gives about 26,700,000 results. </p>

<p><strong>Public Transport</strong> is not really a <em>buzzword</em>, but still on Google you can get almost the same number as with “Big Data”: 26,400,000 results.</p>

<h3 id="why-is-public-transport-so-important">Why is Public Transport so important?</h3>

<p>Because <em>many</em> of us use <a href="http://en.wikipedia.org/wiki/Public_transport">Public Transport</a> every day, but <em>most</em> of us still use their own car to go to work, to bring child at school, and to go shopping. This has a negative impact on the quality of life of everyone and is clearly inefficient since it does cost more: </p>

<ol>
  <li>More money.</li>
  <li>More pollution.</li>
  <li>More time.</li>
</ol>

<p>(Well, for time, it is not always true, but it happens more often than commonly perceived). </p>

<p>Thus, an important <strong>challenge</strong> is to improve the quality of Public Transport while keeping 
its cost competitive. The ultimate goal should be to increase the number of people that <strong>trust</strong> and use Public Transport.</p>

<p>How is it possible to achieve this goal?</p>

<h3 id="transport-operators-are-big-data-producers-are-they">Transport Operators are <strong>Big Data</strong> producers (are they?)</h3>

<p>Modern transport operators have installed so called <em>Automatic Vehicle Monitoring (AVM)</em> systems that use several technologies to monitor the fleet of vehicles that operates the service (e.g., metro coaches, buses, metro trains, trains, …).</p>

<p>The stream of data produced by an AVM might be considered as <strong>Big Data</strong> because of its <strong>volume</strong> and <strong>velocity</strong> (see <a href="https://www.ibm.com/developerworks/community/blogs/jfp/entry/big_data_for_dummies23?lang=en">Big Data For Dummies</a>, by J.F. Puget). Each vehicle produces at regular intervals (measured in seconds) data concerning its position and status. This information is stored in remote data centers. The data for a single day might not be considered as “Big”, however once you start to analyze the historical data, the volume increases significantly. For instance, a public transport operator could easily have around 2000 thousands vehicles that operate 24 hours a day, producing data potentially every second.</p>

<p>At the moment, this stream of data misses the third dimension of Big Data that is <strong>variety</strong>. However, new projects that aim at integrating this information with the stream of data coming from social networks are quickly reaching maturity. One of such project is <a href="http://www.superhub-project.eu/">SuperHub</a>, a FP7 project that has recently won the best exhibit award in Cluster 2 <em>“Smart and sustainable cities for 2020+”</em>, at the <a href="http://www.superhub-project.eu/media-centre/124-ict2013-conference-best-exhibit-award-.html">ICT2013 Conference</a> in Vilnius.</p>

<p>I don’t know whether transport operators are really <strong>Big Data</strong> producers or they are merely <strong>Small Data</strong> producers, but data collected using AVMs are nowadays mainly used to report and monitor the daily activities.</p>

<p>In my own opinion, the data produced by transport operators, integrated with input coming from social networks, should be used to improve the quality of the public transport, for instance, by trying to better tackle <strong>Disruption Management</strong> issues.</p>

<p>So, I am curious:</p>

<blockquote>
  <p>Do you know any project that uses AVM data, combined with Social Network inputs (e.g., from <a href="http://www.twitter.com">Twitter</a>), to elaborate <strong>Disruption Management</strong> strategies for Public Transport? If yes, do they use <strong>Mathematical Optimization</strong> at all?</p>
</blockquote>
]]></content>
  </entry>
  
</feed>
